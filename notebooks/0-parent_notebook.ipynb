{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a19993-aac1-4078-a942-aa8d1d731565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc8dcc60-7597-42ef-b46d-1266da895138",
   "metadata": {},
   "source": [
    "# ML notebooks to process the outputs of [nf-core/metaboigniter](https://github.com/nf-core/metaboigniter) pipeline\n",
    "\n",
    "---\n",
    "\n",
    "As explained in the metaboigniter's GitHub repository (https://github.com/nf-core/metaboigniter/blob/master/docs/output.md), the output folder contains three tabular files, one for the __peak table__, one for the __variable metadata__ (including mz, RT, adduct, isotope, and identification information for each mass trace) and __sample metadata__ (original file names for each sample and additional information provided by the phenotype file).\n",
    "\n",
    "In the following notebooks, we will use the peak table to identify the compounds which most separate the two samples groups (Liver Cancer vs. Case Control).\n",
    "\n",
    "Then, we will use the two other text files (variable and sample metadata) to identify these meaningful variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d929fc-ec15-466c-860d-6ac0ce42a8a1",
   "metadata": {},
   "source": [
    "---\n",
    "### 1-clean_peakTable.ipynb\n",
    "\n",
    "This notebook takes the file `peaktablePOSout_pos_metfrag.txt` from metaboigniter results as input.\n",
    "\n",
    "Initially, the txt file looks like that :\n",
    "\n",
    "| dataMatrix      | EPIC_Liver_Cancer_NR160809_007_41_LivCan_153_007.mzML | EPIC_Liver_Cancer_NR160809_008_41_LivCan_154_008.mzML | ... |\n",
    "| :-------------- | :-----------:| :----------: | :---: |\n",
    "| variable_3      | 19.7617... | 19.7352... | ... |\n",
    "| variable_5      | 14.5368... | 15.1933... | ... |\n",
    "| variable_6      | 22.1855... | 20.8314... | ... |\n",
    "| ...                   | ...              | ...              | ... |\n",
    "\n",
    "It contains variables in rows and samples in columns.\n",
    "\n",
    "For further analysis, we prefer to have variables (i.e. compounds) in columns and samples in rows. Moreover, in this notebook, we add two columns :\n",
    "- SampleID : LivCan-XXX, which is the suffix of the filename\n",
    "- Groups : Incident or Non-case\n",
    "\n",
    "At the end of this notebook, our peak table looks like that :\n",
    "\n",
    "| SampleID               | Groups     | variable_3  | variable_5 | ... |\n",
    "| :--------------------- | :----------: | :----------: | :----------: | :---: |\n",
    "| LivCan_153.mzML | Incident    | 19.7617... | 14.5368... | ... |\n",
    "| LivCan_154.mzML | Non-case | 19.7352... | 15.1933... | ... |\n",
    "| ...                            | ...              | ...              | ...              | ... |\n",
    "\n",
    "The output of this notebook is the csv file `peakTable_HILIC_POS.csv`.\n",
    "\n",
    "_NB : We could also add more metadata information in columns after __Groups__ column_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ed25d-fac7-48e4-96cf-8bf9e49a4453",
   "metadata": {},
   "source": [
    "### 2-explore_data.ipynb\n",
    "\n",
    "This notebook takes the previously created file `peakTable_HILIC_POS.csv` as input.\n",
    "\n",
    "The objective of this notebook is to explore the peak table cleaned in the previous notebook (__1-clean_RPpos_peakTable__) with a few visualisations on :\n",
    "- target (here the Sample Group, i.e. Incident vs. Non-case)\n",
    "- missing values\n",
    "- outliers\n",
    "- ...\n",
    "\n",
    "#### Basic checklist :\n",
    "\n",
    "__Form analysis__ :\n",
    "- __target__ : Groups\n",
    "- __shape (rows & columns)__ : 186 rows (samples) x 558 columns (556 compounds)\n",
    "- __features types__ :\n",
    "    - quantitative : 2 (Group, SampleID)\n",
    "    - qualitative : 556 (compounds)\n",
    "- __missing values__ :\n",
    "    - compounds can be in every sample (0% of missing values), in most of them or just in a few\n",
    "    - the maximum of missing value for a variable is 49.5%, i.e. this variable is absent from 49.5% of the samples\n",
    "    - (samples seem to be more easily separated with not too much missing values (logic))\n",
    "\n",
    "__Content analysis__ :\n",
    "- __target visualisation__ :\n",
    "    - ratio 1:1 (93 Cancer - 93 Healthy)\n",
    "- __feature visualisation__ :\n",
    "    - on the first 10 compounds, most of them follow a normal distribution\n",
    "    - some of them follow a double normal distribution\n",
    "    - maybe one distribution for each class (Healthy vs Cancer) --> hypothesis\n",
    "- __relation features/target__ :\n",
    "    - on the first 10 compounds, we don't see a clear difference of intensity between the Cancer and Healthy samples --> previous hypothesis rejected on these compounds --> may be true of others\n",
    "- __relation features/features__ : strong correlations between some of the features --> need to reduce the dimension for further analysis\n",
    "- __t-test__ : this is a huge approximation but a first t-test allows to have a first view on potential important variables\n",
    "\n",
    "This t-test is a huge approximation as it considers the feature's intensities independant, but we know that they highly interact. We can still observe that, for a few compounds, there exists a significant difference of intensity between the cancer and healthy samples.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd766f1-a4cc-4f6f-8822-541b6fa8c575",
   "metadata": {},
   "source": [
    "### 3-missing_value_imputation.ipynb\n",
    "\n",
    "This notebook takes the csv file `peakTable_HILIC_POS.csv` (created in the notebook `1-clean_peakTable.ipynb`) as input.\n",
    "\n",
    "The purpose of this notebook is to use different methods to fill the missing values in our peak table :\n",
    "\n",
    "- __Univariate__ feature imputation :\n",
    "    - __zero__ (or __one__ or any other constant value to avoid further analytical problems)\n",
    "    - __mean__\n",
    "    - __median__\n",
    "    - __mode__ (most frequent)\n",
    "    - __minimum__\n",
    "    - __half minimum__\n",
    "- __Multivariate__ feature imputation :\n",
    "    - __MICE__ (inspired by the R `MICE` package)\n",
    "- __KNN imputation__\n",
    "\n",
    "These methods come from the scikitlearn documentation : [cf. doc scikitlearn](https://scikit-learn.org/stable/modules/impute.html#marking-imputed-values)\n",
    "\n",
    "One type of imputation algorithm is __univariate__, which imputes values in the i-th feature dimension __using only non-missing values in that feature dimension__ (e.g. `impute.SimpleImputer`>). By contrast, __multivariate__ imputation algorithms __use the entire set of available feature dimensions__ to estimate the missing values (e.g. `impute.IterativeImpute`).\n",
    "\n",
    "\n",
    "The scikitlearn `IterativeImputer` is still experimental, so we will also use directly the R `MICE` package  ([documentation](https://www.rdocumentation.org/packages/mice/versions/3.13.0/topics/mice)) in the separate R notebook `3.2-missing_value_imputation_MICE` (in this directory)\n",
    "\n",
    "Here is a link where the [MICE algorithm is explained](https://cran.r-project.org/web/packages/miceRanger/vignettes/miceAlgorithm.html).\n",
    "\n",
    "The MICE (Multivariate Imputation by Chained Equations) algorithm is a multivariate method to impute missing values. Each missing value is imputed using a separate model with the other variables in the dataset. Iterations should be run until it appears that convergence has been met.\n",
    "\n",
    "\n",
    "For each of these methods, we can save the imputed peak table as a new csv file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c78eb-1994-4384-898b-f23ee812fd3d",
   "metadata": {},
   "source": [
    "### 4-normalisation_scaling_pipeline.ipynb\n",
    "\n",
    "This notebook takes an imputed peak table as input, imputed with any of the previous methods, as long as it has no NAs left.\n",
    "\n",
    "The purpose of this notebook is to use different methods to normalise/scale the data in our peak table.\n",
    "\n",
    "The function `normPeakTable` takes a peak table and a list of methods to normalise the peak table. The available methods are :\n",
    "- log10 : base-10 logarithm\n",
    "- std : standard scaler\n",
    "- min-max normalisation\n",
    "    - minmax : across features\n",
    "    - minmax_rows : across samples\n",
    "- scale to unit norm (vector length). If $x$ is the vector of length $n$, the normalized vector is $y=x/z$ then $z$ is defined as followed according to the chosen norm :\n",
    "    - norm_l1 : with l1 norm $\\rightarrow z = \\| x\\|_1 = \\sum_{i=1}^n |x_i|$\n",
    "    - norm_l2 : with l2 norm $\\rightarrow z = \\| x\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}$\n",
    "    \n",
    "\n",
    "It is possible to apply different normalisation/scaling methods to the peak table. You should provide as parameter the list of the methods in the order you want it to be applied to the peak table. Example, for _normPeakTable(X_KNN, ['std', 'norm_l2'])_, a standard scaler will first be applied, followed by a norm l2 scaling.\n",
    "\n",
    "The normalised/scaled peak tables (output of the function `normPeakTable`) can be saved as csv file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cee32c-2b89-493c-a2d0-dda6a52b0831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce503f1-a046-47cd-9821-7b0fb61d515d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ef08a-71b9-474a-9452-01d929da73e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3731198e-aab6-4932-b372-df1b57f11a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd106205-12e1-4b8a-86d4-fe4783cb583e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
